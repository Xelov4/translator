# Translator - Web Crawler & Data Enrichment Tool

## ğŸ“‹ Description

Outil de crawling web intelligent pour enrichir les donnÃ©es d'outils AI. Le systÃ¨me analyse automatiquement les sites web, classe les rÃ©sultats (OK/ERROR), et gÃ¨re intelligemment la progression avec reprise automatique.

## ğŸš€ FonctionnalitÃ©s

### **Web Crawler Intelligent**
- **Classification automatique** : Sites OK vs ERROR basÃ©e sur les codes HTTP
- **Screenshots** : Capture automatique des pages d'accueil (1920x1080)
- **Nettoyage HTML** : Suppression du CSS pour allÃ©ger les fichiers
- **Multithreading** : Traitement parallÃ¨le configurable (2-4 workers)
- **Gestion d'erreurs** : DÃ©tails des erreurs avec codes de statut

### **Gestion de Progression**
- **Reprise automatique** : Reprend lÃ  oÃ¹ il s'est arrÃªtÃ©
- **Protection des sites OK** : Une fois validÃ©s, jamais re-testÃ©s
- **Re-test intelligent** : Option `--retest-errors` pour re-tester les Ã©checs
- **Statistiques dÃ©taillÃ©es** : Suivi en temps rÃ©el avec estimations

### **DÃ©tails d'Erreur**
- **Codes HTTP** : 404, 403, 500, etc.
- **Types d'erreur** : Serveur, client, rÃ©seau, SSL
- **Messages descriptifs** : Explications claires des problÃ¨mes
- **Historique** : Date et heure des tests

## ğŸ“Š Statistiques Actuelles

- **âœ… Sites OK** : 13,769
- **âŒ Sites ERROR** : 2,993  
- **ğŸ“„ Pages totales** : 39,668
- **ğŸ“ Dossiers** : 16,762 sites traitÃ©s

## ğŸ› ï¸ Installation

```bash
# Cloner le repository
git clone https://github.com/Xelov4/translator.git
cd translator

# Installer les dÃ©pendances
pip install pandas requests beautifulsoup4 selenium webdriver-manager pillow deep-translator
```

## ğŸš€ Utilisation

### **Crawler principal**
```bash
# Traiter tous les nouveaux outils
python global_crawler.py --max-workers 2 --max-pages 5

# Re-tester les sites en erreur
python global_crawler.py --retest-errors --max-workers 2 --max-pages 5

# DÃ©sactiver les screenshots pour plus de vitesse
python global_crawler.py --disable-screenshots --max-workers 4
```

### **Mise Ã  jour des statistiques**
```bash
# Analyser les dossiers et mettre Ã  jour crawler_progress.json
python update_progress.py
```

## ğŸ“ Structure des Fichiers

```
translator/
â”œâ”€â”€ global_crawler.py          # Crawler principal
â”œâ”€â”€ update_progress.py         # Analyse des statistiques
â”œâ”€â”€ tools.csv                  # DonnÃ©es source
â”œâ”€â”€ crawler_progress.json      # Progression (auto-gÃ©nÃ©rÃ©)
â”œâ”€â”€ crawled_sites/
â”‚   â”œâ”€â”€ OK/                   # Sites fonctionnels
â”‚   â””â”€â”€ ERROR/                # Sites en erreur
â””â”€â”€ .gitignore               # Exclusions Git
```

## âš™ï¸ Options du Crawler

- `--retest-errors` : Re-tester uniquement les sites en erreur
- `--max-pages N` : Nombre maximum de pages par site (dÃ©faut: 5)
- `--max-workers N` : Nombre de workers parallÃ¨les (dÃ©faut: 2)
- `--disable-screenshots` : DÃ©sactiver les screenshots pour plus de vitesse

## ğŸ”§ Configuration

Le systÃ¨me utilise un fichier `crawler_progress.json` pour :
- Suivre les sites traitÃ©s
- ProtÃ©ger les sites OK validÃ©s
- GÃ©rer les dÃ©tails d'erreur
- Calculer les statistiques

## ğŸ“ˆ Types d'Erreurs DÃ©tectÃ©es

- **404** : Page non trouvÃ©e
- **403** : AccÃ¨s interdit
- **500** : Erreur serveur
- **SSL** : ProblÃ¨mes de certificat
- **DNS** : RÃ©solution de domaine Ã©chouÃ©e
- **Timeout** : Connexion expirÃ©e
- **Network** : ProblÃ¨mes rÃ©seau

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commit les changements (`git commit -m 'Add AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails. 